{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697e6d1a-6865-4880-addd-01adbdac705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow.image as imtf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import mahotas.features.texture as texture\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c2e1e6-a58b-4f7b-ba77-1b1f6a8a90b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>furniture</th>\n",
       "      <th>style</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./Furniture_Data/tables/Eclectic/4029eclectic-...</td>\n",
       "      <td>table</td>\n",
       "      <td>eclectic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./Furniture_Data/tables/Eclectic/4107eclectic-...</td>\n",
       "      <td>table</td>\n",
       "      <td>eclectic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./Furniture_Data/tables/Eclectic/3885eclectic-...</td>\n",
       "      <td>table</td>\n",
       "      <td>eclectic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./Furniture_Data/tables/Eclectic/4040eclectic-...</td>\n",
       "      <td>table</td>\n",
       "      <td>eclectic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./Furniture_Data/tables/Eclectic/4171eclectic-...</td>\n",
       "      <td>table</td>\n",
       "      <td>eclectic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path furniture     style\n",
       "0  ./Furniture_Data/tables/Eclectic/4029eclectic-...     table  eclectic\n",
       "1  ./Furniture_Data/tables/Eclectic/4107eclectic-...     table  eclectic\n",
       "2  ./Furniture_Data/tables/Eclectic/3885eclectic-...     table  eclectic\n",
       "3  ./Furniture_Data/tables/Eclectic/4040eclectic-...     table  eclectic\n",
       "4  ./Furniture_Data/tables/Eclectic/4171eclectic-...     table  eclectic"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "data_file_path = \"./data/transforms.parquet\"\n",
    "df = pd.read_parquet(data_file_path)\n",
    "\n",
    "# Note: Numpy array is treated as string here \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89efb803-699d-4ae4-921f-04c8c508a5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b10838ddf8b46418d22dedd1049e7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182457 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_image(img_path):\n",
    "    img_arr = cv2.imread(img_path)\n",
    "    \n",
    "    # Downsize image for faster model training. \n",
    "    # Normalize the image to scale 0-1 for faster training time and better performance\n",
    "    return cv2.resize(img_arr, (50,50)) / 255.0 \n",
    "\n",
    "X = df['path'].progress_apply(lambda path: get_image(path)).to_list()\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f812ee-3c69-45ef-a250-c190a4d405e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table': 1, 'sofa': 2, 'lamp': 3, 'chair': 4, 'dresser': 5, 'bed': 6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define furniture index. We'll use this data format as target for training \n",
    "furniture_indexes = {furniture:idx for idx, furniture in enumerate(df['furniture'].unique(), 1)}\n",
    "furniture_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4de35a2-50a4-435f-b713-ad81860355ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(furniture):\n",
    "    return furniture_indexes[furniture]\n",
    "\n",
    "y = df['furniture'].apply(lambda furniture: get_index(furniture)).to_list()\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41cec19c-e566-4217-aae2-e756a305663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "# Using stratify on y to ensure better distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "X_train = np.squeeze(X_train)\n",
    "y_train = np.squeeze(y_train)\n",
    "X_test = np.squeeze(X_test)\n",
    "y_test = np.squeeze(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "918e1651-226d-43f2-a13f-396e02fb70ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (109474, 50, 50, 3)\n",
      "X_test shape (72983, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train shape {X_train.shape}\")\n",
    "print(f\"X_test shape {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a93d001-0b6b-44f2-a39a-d7e19ca4d0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape (109474,)\n",
      "y_test shape (72983,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_train shape {y_train.shape}\")\n",
    "print(f\"y_test shape {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7154e63-c511-416a-b6b5-348dd621a579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 13:58:25.130468: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3422/3422 [==============================] - 81s 24ms/step - loss: 0.5017 - accuracy: 0.8230 - val_loss: 0.3039 - val_accuracy: 0.8983\n",
      "Epoch 2/5\n",
      "3422/3422 [==============================] - 74s 22ms/step - loss: 0.2658 - accuracy: 0.9104 - val_loss: 0.2659 - val_accuracy: 0.9150\n",
      "Epoch 3/5\n",
      "3422/3422 [==============================] - 73s 21ms/step - loss: 0.2088 - accuracy: 0.9304 - val_loss: 0.2210 - val_accuracy: 0.9261\n",
      "Epoch 4/5\n",
      "3422/3422 [==============================] - 66s 19ms/step - loss: 0.1738 - accuracy: 0.9420 - val_loss: 0.2074 - val_accuracy: 0.9320\n",
      "Epoch 5/5\n",
      "3422/3422 [==============================] - 66s 19ms/step - loss: 0.1496 - accuracy: 0.9489 - val_loss: 0.1728 - val_accuracy: 0.9444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2876b3cd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Create a sequential model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "# model.add(layers.Conv2D(8, (3, 3), activation='relu', input_shape=(224, 224,3),kernel_regularizer=regularizers.l2(0.001)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(50, 50,3), padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(512, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(1024, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(512*2*2, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "# Add a dense layer\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "# model.add(layers.Dropout(0.2)) # add a dropout layer with dropout rate of 0.2\n",
    "\n",
    "\n",
    "# Add the output layer\n",
    "model.add(layers.Dense(64, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c269918-c40e-4d2a-8617-d4c8a16ba4d5",
   "metadata": {},
   "source": [
    "### Calculate similarity features for later similarity computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1986e5e-ce1a-44cd-a2ac-3d16950b677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 88\n",
    "\n",
    "def get_histogram(image: np.ndarray):\n",
    "    histogram = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n",
    "    histogram = cv2.normalize(histogram, histogram).flatten()\n",
    "    return histogram \n",
    "\n",
    "def get_texture_feature(image: np.ndarray):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    glcm = texture.haralick(gray_image)\n",
    "    return np.mean(glcm, axis=0)\n",
    "\n",
    "def get_compactness(image: np.ndarray):\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply adaptive thresholding\n",
    "    thresholded = cv2.adaptiveThreshold(gray_image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(thresholded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate compactness for each contour\n",
    "    compactness_values = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        compactness = perimeter / np.sqrt(area) if area > 0 else 0\n",
    "        compactness_values.append(compactness)\n",
    "    \n",
    "    # Maximum Compactness: Choose the contour with the highest compactness value. \n",
    "    # This approach assumes that the object with the highest compactness is the most significant or relevant in the image.\n",
    "    max_compactness_index = np.argmax(compactness_values)\n",
    "    max_compactness_value = compactness_values[max_compactness_index]\n",
    "    \n",
    "    return max_compactness_value\n",
    "\n",
    "\n",
    "# def get_similarity_attrs(row: pd.Series) -> pd.Series: \n",
    "#     print(row['path'])\n",
    "#     image = cv2.imread(row['path'])\n",
    "\n",
    "#     row['histogram'] = get_histogram(image)\n",
    "#     row['texture_feature'] = get_texture_feature(image)\n",
    "#     row['compactness'] = get_compactness(image)\n",
    "\n",
    "#     return row \n",
    "\n",
    "def get_similarity_attrs(path):\n",
    "    image = cv2.imread(path)\n",
    "    histogram = get_histogram(image)\n",
    "    texture_feature = get_texture_feature(image)\n",
    "    compactness = get_compactness(image)\n",
    "    return pd.Series([histogram, texture_feature, compactness], index=['histogram', 'texture_feature', 'compactness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f394ab1-1917-47e6-93ee-332577e7be8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns to store the results (take abt 1 hour to complete)\n",
    "df[['histogram', 'texture_feature', 'compactness']] = df['path'].progress_apply(get_similarity_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646152d-1075-4478-aa3b-712f9d4ad956",
   "metadata": {},
   "source": [
    "## Post execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bfe90-92c7-4a60-84b0-303bd2f89737",
   "metadata": {},
   "source": [
    "Save new dataset curated for similarity score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69f871-b5c0-4acd-981b-0be87cd95b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the model\n",
    "def save_dataset(df: pd.DataFrame, data_file_name: str) -> None:\n",
    "    data_dir = \"./data\"\n",
    "    data_file_path = os.path.join(data_dir, data_file_name)\n",
    "    \n",
    "    # Check if the directory exists, and if not, create it\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    df.to_parquet(data_file_path, index=False, engine='pyarrow')\n",
    "    print(f\"Saved DataFrame to {data_file_path}\")\n",
    "\n",
    "data_file_name = \"similarities.parquet\"\n",
    "save_dataset(dummy, data_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6adde5-fef4-48d9-a23d-f4ea48b307b0",
   "metadata": {},
   "source": [
    "Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49990b6e-2f01-4a84-8dfa-dda4ef34464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./models\"\n",
    "model_h5_file = os.path.join(model_dir, \"model_task_2_cnn_classification.h5\")\n",
    "\n",
    "# Check if the directory exists, and if not, create it\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "model.save(model_h5_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
